#!/usr/bin/env python

#
# - Split a fasta file.
# - Create an index.
# - Generate Array Job QSUB.
#

import argparse
import logging
import itertools
import os

from Bio import SeqIO


def parse_args():
    ''' return arguments
        >>> args = parse_args()
    '''

    parser = argparse.ArgumentParser()
    parser.add_argument('--log', default='/dev/stderr', help='log file (default=stderr)')
    parser.add_argument('--verbose', default=False, action='store_true')
    parser.add_argument('--chunk-size', default=10000, type=int, help='number of sequences per chunk')
    parser.add_argument('--directory', type=str, required=True, help='chunk and job file output directory')
    parser.add_argument('--fasta-file', type=str, required=True, help='input fasta file', default='/dev/stdin')
    parser.add_argument('--post-chunk-script', type=str, default=None, help='run script after each chunk is written'
            ' with the output filename in the environment variable OUT_FILE')

    return parser.parse_args()


def post_chunk_script(command, out_file):
    ''' Run the post chunk script '''

    logging.info(command)
    os.environ['OUT_FILE'] = out_file
    status = os.system(command)

    logging.info('exit status: "%s"' % status)

    return status


def setup_logging(logfile='/dev/stderr', verbose=False):

    if verbose:
        level = logging.DEBUG
    else:
        level = logging.INFO

    return logging.basicConfig(filename=logfile, level=level)


def chunk_emitter(iterable, size):
    it = iter(iterable)
    item = list(itertools.islice(it, size))
    while item:
        yield item
        item = list(itertools.islice(it, size))


def main():
    '''
        >>> main() # stuff happens
    '''

    args = parse_args()

    setup_logging(logfile=args.log, verbose=args.verbose)

    logging.info('args=%s' % args)

    chunk_index = {}

    try:
        os.mkdir(args.directory)
    except OSError:
        logging.info('warning! directory exists')

    with open(args.fasta_file) as handle:
        logging.info('reading from %s' % args.fasta_file)
        records = SeqIO.parse(handle, 'fasta')
        chunks = chunk_emitter(records, args.chunk_size)

        for i, chunk in enumerate(chunks):

            logging.info('writing chunk %s' % i)

            output_file = os.path.join(args.directory, 'chunk-%s.fasta' % i)

            chunk_index[i] = output_file

            with open(output_file, 'w') as output_handle:
                SeqIO.write(chunk, output_handle, 'fasta')

    index_file = os.path.join(args.directory, 'index.txt')
    logging.info('saving index to: %s' % index_file)
    with open(index_file, 'w') as handle:
        for k in chunk_index:
            print >> handle, '%s,%s' % (k, chunk_index[k])

if __name__ == '__main__':
    main()


#!/usr/bin/env python

import pandas as pd

import argparse
import logging
from itertools import izip, izip_longest
from collections import defaultdict

def parse_args():

  parser = argparse.ArgumentParser()

  parser.add_argument('--taxonomies', required=True)
  parser.add_argument('--rank', required=True)
  parser.add_argument('--uc-file', default='/dev/stdin')
  parser.add_argument('--output', default='/dev/stdout')
  parser.add_argument('--skipped-table', default='skipped.csv')
  parser.add_argument('--min-length', default=0, type=int)

  return parser.parse_args()


def uc_parser(handle):
    for line in handle:
      try:
        uc = parse_uc(line)
      except:
        logging.info('weird line: %s' % line.strip())
        uc = { 'type': 'W' }
      yield uc


def parse_uc(uc_line):
    ''' parses a line of a uc file returning
    '''

    uc_line = uc_line.strip().split("\t")

    sample = uc_line[8].split()[0]

    # count non-hits as unclassified
    if uc_line[0] == 'N':
        cluster = 'unclassified_reads'
        length = None
        identity = None
    elif uc_line[0] == 'H':
        cluster = uc_line[9]
        length = int(uc_line[2])
        identity = float(uc_line[3])

    return { 'type': uc_line[0],
             'sample': sample,
             'cluster': cluster,
             'identity': identity,
             'length': length }



def grouper(iterable, n, fillvalue=None):
    args = [iter(iterable)] * n
    return izip_longest(*args, fillvalue=fillvalue)


def main():

  logging.basicConfig(logfile='/dev/stderr', level=logging.INFO)

  args = parse_args()

  logging.info('reading taxonomies from %s', args.taxonomies)

  tax = pd.read_csv(args.taxonomies, index_col=0)
  tax = tax[pd.notnull(tax[args.rank])]

  id_to_rank = { str(i[0]): i[1] for i in izip(tax.index, tax[args.rank]) }

  logging.info('loaded %s taxonomies, e.g: %s' % (len(id_to_rank), id_to_rank.values()[0]))
  logging.info('reading uc file %s' % args.uc_file)

  sample_cluster_count = defaultdict(lambda: defaultdict(int))

  skipped = defaultdict(lambda: defaultdict(int))

  with open(args.uc_file) as handle:
    counted, i = 0, 0

    for r1, r2 in grouper(uc_parser(handle), 2):

      i += 1

      if i%100000 == 0:
        logging.info('on line %s, samples=%s, pairs_counted=%s (%.2f%%)' % (i, len(sample_cluster_count.keys()), counted, 100*counted/float(i)))

      # skip misses and "weird" lines
      if not (r1['type'] == 'H' and r2['type'] == 'H'):
          continue

      # skip short alignments
      if not r1['length'] >= args.min_length:
          continue

      t1 = r1['cluster']
      t2 = r2['cluster']

      if not r1['sample'] == r2['sample']:
          logging.info('sample names do not match: %s, %s (line=%s)' % (r1['sample'], r2['sample'], i))
          logging.info('going to try skipping a line')
          handle.next()

      n1 = id_to_rank.get(t1)
      n2 = id_to_rank.get(t2)

      if n1 and n2 and n1 == n2:
        sample_cluster_count[r1['sample']][t1] += 1
        counted += 1
      else:
        skipped[n1][n2] += 1


  otus = pd.DataFrame.from_dict(sample_cluster_count)

  otus.to_csv(args.output)

  skip_table = pd.DataFrame.from_dict(skipped)
  
  skip_table.to_csv(args.skipped_table)



if __name__ == '__main__':
  main()
